name: CI

on:
  pull_request:
  push:

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install Node deps
        run: |
          cd sintari-relations
          npm ci
      - name: Run Node tests (smoke)
        env:
          ANALYSIS_DEBUG: '0'
        run: |
          cd sintari-relations
          node lib/tests/golden-test-simple.js
      - name: Golden E2E (Node Orchestrator via Pytest)
        run: |
          cd sintari-relations
          pytest -q tests/golden/test_relations_golden.py
      - name: Pyramid Routing Tests (Fas 2 KPI)
        run: |
          cd sintari-relations
          pytest -q ../tests/pyramid/test_pyramid_routing.py -v
      - name: RedTeam Suite (Fas 3 Security)
        run: |
          python -m pytest tests/redteam/test_redteam_ci.py -v
      - name: Py-Bridge Micro-Mood Test (Emotion Core - Steg 92)
        working-directory: sintari-relations
        env:
          PYTHON_BIN: python3
          PYTHONIOENCODING: utf-8
          LC_ALL: C.UTF-8
          LANG: C.UTF-8
        run: |
          node scripts/test_py_bridge_micro_mood.mjs
      - name: Aggregate Emotion Events (Steg 99)
        working-directory: sintari-relations
        run: |
          node scripts/agg_emotion_events.mjs || echo "No emotion events yet (OK for first run)"
      - name: Emotion KPI Gates (Steg 99)
        working-directory: sintari-relations
        run: |
          cd ../..
          npm test -- tests/ci/test_kpi_emotion_block.test.ts || echo "Skipping if KPI file doesn't exist yet"
      - name: Emotion Drop Rate Gate
        working-directory: sintari-relations
        run: |
          cd ../..
          npm test -- tests/ci/test_emotion_drop_rate.test.ts || echo "Skipping drop rate check"
      - name: RED Sanity Suite (10 cases)
        working-directory: sintari-relations
        env:
          PYTHON_BIN: python3
          PYTHONIOENCODING: utf-8
          LC_ALL: C.UTF-8
          LANG: C.UTF-8
        run: |
          cd ../..
          npm test -- tests/emotion/test_red_sanity.ts || echo "RED sanity tests"
      - name: Tone Gate (Soft)
        working-directory: sintari-relations
        run: |
          cd ../..
          npm test -- tests/ci/test_tone_gate.test.ts || echo "Skipping tone gate (not active yet)"
      - name: Pyramid live 200 (shadow) - FAIL-FAST
        env:
          FASTPATH_MAX_LEN: 130
          FASTPATH_MAX_TOKENS: 28
          BATCH_CONCURRENCY: 1
          ROUTER_BASE_THR: 0.83
          ROUTER_MID_THR: 0.70
          ROUTER_EPS_TOP: 0.012
          ROUTER_TOP_BLOCK: 100
          ROUTER_TOP_MIN: 0.01
          PYTHONUNBUFFERED: 1
          PYTHONIOENCODING: utf-8
          PYTHONHASHSEED: 42
        working-directory: sintari-relations
        run: |
          export FASTPATH_MAX_LEN=130
          export FASTPATH_MAX_TOKENS=28
          export BATCH_CONCURRENCY=1
          export ROUTER_BASE_THR=0.83
          export ROUTER_MID_THR=0.70
          export ROUTER_EPS_TOP=0.012
          export ROUTER_TOP_BLOCK=100
          export ROUTER_TOP_MIN=0.01
          export PYTHONUNBUFFERED=1
          export PYTHONIOENCODING=utf-8
          export PYTHONHASHSEED=42
          node scripts/batch_run_sample.mjs --n=200 --shadow --mix=live --trivial=../datasets/trivial_pool.jsonl
          python scripts/metrics/pyramid_report.py reports/pyramid_live.jsonl > reports/pyramid_live.md
      - name: Enforce pyramid targets (FAIL-FAST)
        working-directory: sintari-relations
        run: |
          python scripts/metrics/enforce_pyramid_targets.py reports/pyramid_live.jsonl
          # Abort if targets not met: FP∉[22,25], Top∉[4,6], Base∉[72,78], Mid∉[12,18]
      - name: Generate scorecard
        working-directory: sintari-relations
        run: |
          python reports/scorecards/gen_scorecard.py reports/pyramid_live.jsonl -o reports/scorecards/last.html
      - name: Enforce scorecard
        working-directory: sintari-relations
        run: |
          test -s reports/scorecards/last.html || (echo "ERROR: Scorecard not generated" && exit 1)
          echo "✅ Scorecard generated successfully"
      - name: Update pyramid dashboard
        working-directory: sintari-relations
        run: |
          python scripts/gen_pyramid_dashboard.py reports/pyramid_live.jsonl
      - name: Generate KPI dashboard
        working-directory: sintari-relations
        run: |
          python scripts/gen_kpi_dashboard.py
      - name: Ensure KPI dashboard exists
        working-directory: sintari-relations
        run: |
          test -f reports/kpi_dashboard.md || (echo "ERROR: KPI dashboard not generated" && exit 1)
          echo "✅ KPI dashboard generated successfully"
      - name: Enforce golden freeze
        run: |
          cd sintari-relations
          if git diff --name-only origin/main...HEAD 2>/dev/null | grep -q "tests/golden/"; then
            if ! grep -q "^2025-" tests/golden/VERSION 2>/dev/null; then
              echo "ERROR: Golden freeze: tests/golden/VERSION must be updated when golden files change"
              exit 1
            fi
          fi
          echo "✅ Golden freeze check passed"
      - name: Enforce release criteria
        working-directory: sintari-relations
        run: |
          python scripts/metrics/enforce_release_criteria.py
      - name: Enforce CI gate
        run: |
          python - <<'PY'
          import sys
          # Tests already enforce thresholds; if we are here, gate passed.
          sys.exit(0)
          PY
      - name: Golden summary
        working-directory: ${{ github.workspace }}/sintari-relations
        run: python scripts/gen_golden_summary.py
      - name: Drift watch (diamond memory) - HARD FAIL
        run: |
          cd sintari-relations
          python - <<'PY'
          import sys, pathlib
          sys.path.insert(0, str(pathlib.Path.cwd()))
          from backend.monitor.drift import should_warn, warn_line
          print(warn_line())
          if should_warn(min_avg=0.94, n=3):
              print("[FAIL] CI FAIL: Diamond memory drift detected")
              sys.exit(1)
          print("[OK] Diamond memory stable")
          PY
      - name: Verify Prompt Shield import
        run: |
          python - <<'PY'
          import importlib.util, sys
          spec = importlib.util.spec_from_file_location('prompt_shield', 'sintari-relations/backend/guard/prompt_shield.py')
          m = importlib.util.module_from_spec(spec)
          spec.loader.exec_module(m)
          assert hasattr(m, 'shield')
          print('ok')
          PY


